{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255d6971",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/algebra_y_trigonometria/blob/main/classes/0_formatos_clase/algebra_and_trigonometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/algebra_y_trigonometria/blob/main/classes/0_formatos_clase/algebra_and_trigonometry.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669f881",
   "metadata": {},
   "source": [
    "# Prompt para el dise√±o de la metodolog√≠a de consolidaci√≥n de datos meteorol√≥gicos y epidemiol√≥gicos: \n",
    "\n",
    "Emulando a un experto en an√°lisis de series temporales y modelado estad√≠stico, especializado en la consolidaci√≥n de datos meteorol√≥gicos y epidemiol√≥gicos. Dise√±a una metodolog√≠a detallada para consolidar estos datos meteorol√≥gicos y epidemiol√≥gicos de Caucasia Antioquia (Colombia) para ser modelados con SARIMAX, asegurando su calidad y relevancia para futuros an√°lisis. La metodolog√≠a debe incluir los siguientes pasos:   \n",
    "\n",
    "\n",
    "1. **Recolecci√≥n de Datos**: Describe c√≥mo y d√≥nde se obtendr√°n los datos meteorol√≥gicos (por ejemplo, temperatura, humedad, precipitaciones) y epidemiol√≥gicos (por ejemplo, tasas de infecci√≥n, hospitalizaciones) de fuentes confiables. Incluye consideraciones sobre la frecuencia de los datos (diaria, semanal, mensual) y el per√≠odo de tiempo a cubrir.  \n",
    "2. **Limpieza de Datos**: Explica los procedimientos para identificar y manejar datos faltantes, valores at√≠picos y errores de medici√≥n. Detalla las t√©cnicas que se utilizar√°n para imputar datos faltantes y corregir errores, asegurando la integridad de los conjuntos de datos.  \n",
    "3. **Transformaci√≥n de Datos**: Describe c√≥mo se transformar√°n los datos para facilitar su an√°lisis. Esto puede incluir la normalizaci√≥n, estandarizaci√≥n, creaci√≥n de variables derivadas (por ejemplo, medias m√≥viles, √≠ndices de calor) y la conversi√≥n de formatos de fecha y hora.  \n",
    "4. **Integraci√≥n de Datos**: Explica c√≥mo se combinar√°n los datos meteorol√≥gicos y epidemiol√≥gicos en un solo conjunto de datos. Detalla los m√©todos para alinear temporalmente los datos, manejar diferentes frecuencias de muestreo y asegurar la coherencia entre las variables.  \n",
    "5. **An√°lisis Exploratorio de Datos (EDA)**: Describe las t√©cnicas que se utilizar√°n para explorar los datos consolidados, identificar patrones, tendencias y relaciones entre las variables meteorol√≥gicas y epidemiol√≥gicas. Incluye visualizaciones recomendadas y estad√≠sticas descriptivas.  \n",
    "6. **Documentaci√≥n y Reproducibilidad**: Explica c√≥mo se documentar√° cada paso del proceso de consolidaci√≥n de datos para asegurar la reproducibilidad. Incluye recomendaciones sobre el uso de herramientas de control de versiones y la creaci√≥n de informes detallados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97276c7c",
   "metadata": {},
   "source": [
    "# Metodolog√≠a hasta hoy 21 de Septiembre de 2025  \n",
    "\n",
    "Esta es una metodolog√≠a pr√°ctica, detallada y reproducible (emulando a un experto en series temporales y modelado SARIMAX) para consolidar datos meteorol√≥gicos y epidemiol√≥gicos de **Caucasia, Antioquia (Colombia)**, lista para ejecutar y reutilizar por tu equipo de investigaci√≥n. Incluyo recomendaciones t√©cnicas, criterios de calidad y pasos concretos para cada etapa solicitada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d352ed",
   "metadata": {},
   "source": [
    "\n",
    "# Metodolog√≠a para consolidaci√≥n de datos meteorol√≥gicos y epidemiol√≥gicos (orientada a SARIMAX)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5260f92",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Recolecci√≥n de Datos\n",
    "\n",
    "**Objetivo**: reunir series temporales completas y bien documentadas de variables meteorol√≥gicas y epidemiol√≥gicas.\n",
    "\n",
    "**Fuentes recomendadas (ejemplos pr√°cticos para Caucasia)**\n",
    "\n",
    "* **Meteorolog√≠a**: IDEAM (estaciones meteorol√≥gicas nacionales), proyectos regionales como *Antioquia ‚Äî Mira su Cielo*, repositorios acad√©micos o redes agr√≠colas (p. ej. datos de Fedearroz si disponibles para lluvia/temperatura).\n",
    "* **Epidemiolog√≠a**: Secretar√≠a de Salud Municipal de Caucasia (casos notificados, hospitalizaciones), Secretar√≠a Seccional de Salud de Antioquia, INS (boletines nacionales), registros hospitalarios locales (EPS/hospital), SIVIGILA (cuando aplique).\n",
    "* **Metadatos y geolocalizaci√≥n**: fichas de estaci√≥n IDEAM (altura, lat/long, cambios de sensor), documentaci√≥n de protocolos de muestreo epidemiol√≥gico (definici√≥n de caso, fecha de inicio, fecha de notificaci√≥n).\n",
    "\n",
    "**Frecuencia y per√≠odo recomendado**\n",
    "\n",
    "* **Frecuencia primaria**: **diaria** (ideal para meteorolog√≠a y para modelar incidencia con retardo).\n",
    "* **Agrupaciones**: conservar la granularidad diaria y proporcionar res√∫menes semanales/mensuales seg√∫n necesidad.\n",
    "* **Per√≠odo m√≠nimo aconsejado**: **>10 a√±os**; si es posible usar **2020‚Äì2025 (‚â•5 a√±os)** para capturar variabilidad interanual y estacional ‚Äî esto coincide con trabajos previos en la regi√≥n y mejora la robustez del SARIMAX.\n",
    "* **Fecha y zona horaria**: registrar en **ISO 8601** (YYYY-MM-DD) y usar **America/Bogota (UTC-5)** consistentemente.\n",
    "\n",
    "**Pr√°ctica operativa**\n",
    "\n",
    "* Automatizar descargas (APIs, FTP, scrapers controlados) y almacenar **raw/** sin modificar.\n",
    "* Registrar el *hash* (SHA256) de cada archivo crudo y la fecha de extracci√≥n.\n",
    "* Mantener un inventario (CSV/JSON) con: fuente, variable, frecuencia, periodo, contacto, licencias.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1694de4",
   "metadata": {},
   "source": [
    "Listo ‚Äî gener√© y guard√© un script (ejecutado aqu√≠) que crea datos artificiales diarios para **2020-01-01 ‚Üí 2025-12-31** y produce un paquete \"raw\" con metadatos e inventario. Abajo te dejo:\n",
    "\n",
    "1. El **script completo** (para copiar/pegar y ejecutar localmente).\n",
    "2. Un resumen de lo que genera y los archivos de salida, incluyendo la ruta del ZIP con todos los CSV raw.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bf822",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Script (copia ‚Äî es el mismo que ejecut√© aqu√≠)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_3700\\1680677275.py:73: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  temp_vals = pd.read_csv(OUT_DIR / f\"meteorology_{stations[0]['station_id']}.csv\", parse_dates=[\"date\"])[\"tmean_C\"].fillna(method=\"ffill\").values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generaci√≥n completa. Archivos en: raw\n",
      "ZIP: caucasia_synthetic_raw_2020_2025.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generaci√≥n de datos sint√©ticos para \"Recolecci√≥n de Datos\" (Caucasia, Antioquia)\n",
    "# Crea datos meteorol√≥gicos diarios (2 estaciones) y datos epidemiol√≥gicos diarios\n",
    "# (casos y hospitalizaciones a nivel municipal) para 2020-01-01 a 2025-12-31.\n",
    "# Guarda archivos en ./raw/ y crea inventory.json con hashes SHA256.\n",
    "\n",
    "import os, json, hashlib, zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "np.random.seed(42)\n",
    "\n",
    "OUT_DIR = Path(\"raw\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq=\"D\", tz=\"America/Bogota\") # genera fechas diarias\n",
    "n = len(dates)\n",
    "\n",
    "def seasonal_temp(day_of_year, base=26.0, amp=6.0):\n",
    "    return base + amp * np.sin(2 * np.pi * (day_of_year / 365.25 - 0.25))\n",
    "\n",
    "def seasonal_precip(day_of_year, base=3.0, amp=3.0):\n",
    "    val = base + amp * np.cos(2 * np.pi * (day_of_year / 365.25 - 0.1))\n",
    "    return np.clip(val, 0.0, None)\n",
    "\n",
    "def sha256_of_file(path): # calcula SHA256 de un archivo \n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "stations = [\n",
    "    {\"station_id\": \"IDEAM_Caucasia_001\", \"name\": \"IDEAM Caucasia - Estaci√≥n Principal\", \"lat\": 7.9320, \"lon\": -75.1980, \"elevation_m\": 45, \"source\": \"IDEAM\", \"notes\": \"Sensor autom√°tico. Calibraci√≥n registrada 2022-06-15.\"},\n",
    "    {\"station_id\": \"MiraCielo_002\", \"name\": \"Antioquia - Mira su Cielo - Estaci√≥n Caucasia\", \"lat\": 7.9350, \"lon\": -75.1925, \"elevation_m\": 48, \"source\": \"Antioquia - Mira su Cielo\", \"notes\": \"Proyecto regional. Cambio de sensor el 2021-11-02.\"}\n",
    "]\n",
    "\n",
    "# Generar meteorolog√≠a por estaci√≥n\n",
    "for st in stations:\n",
    "    day_of_year = dates.dayofyear.values\n",
    "    t_mean = seasonal_temp(day_of_year, base=26.5, amp=4.5)\n",
    "    noise = np.random.normal(scale=1.2, size=n)\n",
    "    t_mean = t_mean + noise + (0.01 * (dates.year - 2020))\n",
    "    t_min = t_mean - np.abs(np.random.normal(loc=4.0, scale=0.8, size=n))\n",
    "    t_max = t_mean + np.abs(np.random.normal(loc=4.0, scale=0.9, size=n))\n",
    "    humidity = np.clip(75 - 5 * np.sin(2 * np.pi * day_of_year/365.25) + np.random.normal(scale=6, size=n), 20, 100)\n",
    "    precip_base = seasonal_precip(day_of_year, base=3.5, amp=4.0)\n",
    "    precip = np.random.poisson(lam=np.clip(precip_base / 2.0, 0.01, None), size=n).astype(float)\n",
    "    heavy_idx = np.random.choice(n, size=int(0.02 * n), replace=False)\n",
    "    precip[heavy_idx] += np.random.gamma(shape=3.0, scale=10.0, size=len(heavy_idx))\n",
    "    wind = np.clip(np.random.normal(loc=2.8, scale=1.1, size=n), 0, None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"tmin_C\": np.round(t_min, 2),\n",
    "        \"tmean_C\": np.round(t_mean, 2),\n",
    "        \"tmax_C\": np.round(t_max, 2),\n",
    "        \"rh_pct\": np.round(humidity, 1),\n",
    "        \"precip_mm\": np.round(precip, 2),\n",
    "        \"wind_m_s\": np.round(wind, 2),\n",
    "    })\n",
    "    df[\"station_id\"] = st[\"station_id\"]\n",
    "    if st[\"station_id\"].endswith(\"_002\"):\n",
    "        mask_gap = (df[\"date\"] >= \"2021-11-02\") & (df[\"date\"] <= \"2021-11-12\")\n",
    "        df.loc[mask_gap, [\"tmin_C\", \"tmean_C\", \"tmax_C\", \"rh_pct\", \"precip_mm\", \"wind_m_s\"]] = np.nan\n",
    "    df.to_csv(OUT_DIR / f\"meteorology_{st['station_id']}.csv\", index=False)\n",
    "\n",
    "# Generar epidemiolog√≠a municipal (ligada a meteorolog√≠a)\n",
    "precip_station0 = pd.read_csv(OUT_DIR / f\"meteorology_{stations[0]['station_id']}.csv\", parse_dates=[\"date\"])\n",
    "precip_vals = precip_station0[\"precip_mm\"].fillna(0).values\n",
    "temp_vals = pd.read_csv(OUT_DIR / f\"meteorology_{stations[0]['station_id']}.csv\", parse_dates=[\"date\"])[\"tmean_C\"].fillna(method=\"ffill\").values\n",
    "\n",
    "precip_14d = pd.Series(precip_vals).rolling(window=14, min_periods=1).sum().values\n",
    "temp_7d = pd.Series(temp_vals).rolling(window=7, min_periods=1).mean().values\n",
    "risk_index = (precip_14d / (np.percentile(precip_14d, 90)+1)) + ((temp_7d - np.mean(temp_7d)) / (np.std(temp_7d)+1))\n",
    "risk_index = (risk_index - np.nanmin(risk_index)) / (np.nanmax(risk_index) - np.nanmin(risk_index) + 1e-9)\n",
    "\n",
    "baseline = 5 + 3 * np.sin(2 * np.pi * (pd.to_datetime(dates).dayofyear / 365.25))\n",
    "seasonal_component = baseline + 8 * risk_index\n",
    "lam = np.clip(np.abs(np.array(seasonal_component, dtype=float)), 0.1, None)\n",
    "outbreak_days = np.random.choice(n, size=int(0.01 * n), replace=False)\n",
    "lam[outbreak_days] = lam[outbreak_days] + np.random.uniform(20, 80, size=len(outbreak_days))\n",
    "\n",
    "cases = np.random.poisson(lam=lam).astype(int)\n",
    "hosp_rate = np.clip(0.05 + 0.02 * (risk_index), 0.02, 0.25)\n",
    "hospitalizations = np.random.binomial(n=cases, p=hosp_rate).astype(int)\n",
    "\n",
    "df_epi = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"cases\": cases,\n",
    "    \"hospitalizations\": hospitalizations,\n",
    "    \"population\": 140000\n",
    "})\n",
    "mask_missing = (df_epi[\"date\"] >= \"2022-08-10\") & (df_epi[\"date\"] <= \"2022-08-17\")\n",
    "df_epi.loc[mask_missing, \"cases\"] = np.nan\n",
    "df_epi.to_csv(OUT_DIR / \"epidemiology_caucasia_municipal.csv\", index=False)\n",
    "\n",
    "# Stations metadata and inventory\n",
    "meta_stations = pd.DataFrame(stations)\n",
    "meta_stations.to_csv(OUT_DIR / \"stations_metadata.csv\", index=False)\n",
    "\n",
    "inventory = []\n",
    "for p in list(OUT_DIR.glob(\"*.csv\")):\n",
    "    sha = sha256_of_file(p)\n",
    "    if \"meteorology\" in p.name:\n",
    "        var = \"meteorology\"\n",
    "    elif \"epidemiology\" in p.name:\n",
    "        var = \"epidemiology\"\n",
    "    else:\n",
    "        var = \"metadata\"\n",
    "    df_tmp = pd.read_csv(p, parse_dates=[\"date\"]) if \"date\" in pd.read_csv(p, nrows=1).columns else None\n",
    "    period_from, period_to = None, None\n",
    "    if df_tmp is not None and \"date\" in df_tmp.columns:\n",
    "        period_from = str(df_tmp[\"date\"].min())\n",
    "        period_to = str(df_tmp[\"date\"].max())\n",
    "    inventory.append({\n",
    "        \"file\": str(p),\n",
    "        \"variable\": var,\n",
    "        \"frequency\": \"daily\",\n",
    "        \"period_from\": period_from,\n",
    "        \"period_to\": period_to,\n",
    "        \"sha256\": sha,\n",
    "        \"source\": \"synthetic_simulation\",\n",
    "        \"contact\": \"Marco - ejemplo sint√©tico\",\n",
    "        \"license\": \"CC-BY-4.0 (simulated)\"\n",
    "    })\n",
    "\n",
    "with open(OUT_DIR / \"inventory.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inventory, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ZIP para distribuci√≥n\n",
    "zip_path = Path(\"caucasia_synthetic_raw_2020_2025.zip\")\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in OUT_DIR.glob(\"*\"):\n",
    "        zf.write(p, arcname=p.name)\n",
    "\n",
    "print(\"Generaci√≥n completa. Archivos en:\", OUT_DIR)\n",
    "print(\"ZIP:\", zip_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9e1a9",
   "metadata": {},
   "source": [
    "Este es un **script en Python** que puedes correr directamente en un cuaderno Jupyter abierto con **VS Code**. El script:\n",
    "\n",
    "1. Carga los archivos sint√©ticos (los `.csv` generados).\n",
    "2. Muestra un vistazo r√°pido a los datos.\n",
    "3. Realiza visualizaciones b√°sicas con **matplotlib** y **seaborn**.\n",
    "4. Usa **pandas** para resumir semanal/mensual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264007c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualizaci√≥n de datos sint√©ticos: Caucasia\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuraci√≥n de estilo\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Cargar datos meteorol√≥gicos\n",
    "# -------------------------------\n",
    "CARPETA =   r\"C:\\Users\\marco\\Documentos\\investigacion\\arima\\1_programa_trabajo\\2_ejecucion_plan_trabajo\\1_dataset_artificial_para_aprendizaje\\raw\"  \n",
    "ideam = pd.read_csv(CARPETA+\"\\meteorology_IDEAM_Caucasia_001.csv\", \\\n",
    "    parse_dates=[\"date\"])\n",
    "amsc = pd.read_csv(CARPETA+\"\\meteorology_MiraCielo_002.csv\", \\\n",
    "    parse_dates=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db9214e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC-05:00]",
         "type": "unknown"
        },
        {
         "name": "tmin_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmean_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmax_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rh_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_m_s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0f74ecea-9a20-4418-8e9b-fba73e0a589f",
       "rows": [
        [
         "0",
         "2020-01-01 00:00:00-05:00",
         "16.98",
         "22.6",
         "26.05",
         "71.3",
         "5.0",
         "2.19",
         "IDEAM_Caucasia_001"
        ],
        [
         "1",
         "2020-01-02 00:00:00-05:00",
         "18.93",
         "21.84",
         "25.47",
         "68.8",
         "2.0",
         "4.53",
         "IDEAM_Caucasia_001"
        ],
        [
         "2",
         "2020-01-03 00:00:00-05:00",
         "18.63",
         "22.78",
         "26.17",
         "76.3",
         "6.0",
         "3.33",
         "IDEAM_Caucasia_001"
        ],
        [
         "3",
         "2020-01-04 00:00:00-05:00",
         "20.37",
         "23.84",
         "26.55",
         "63.9",
         "3.0",
         "1.34",
         "IDEAM_Caucasia_001"
        ],
        [
         "4",
         "2020-01-05 00:00:00-05:00",
         "17.39",
         "21.74",
         "25.87",
         "72.8",
         "6.0",
         "3.15",
         "IDEAM_Caucasia_001"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tmin_C</th>\n",
       "      <th>tmean_C</th>\n",
       "      <th>tmax_C</th>\n",
       "      <th>rh_pct</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>wind_m_s</th>\n",
       "      <th>station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00-05:00</td>\n",
       "      <td>16.98</td>\n",
       "      <td>22.60</td>\n",
       "      <td>26.05</td>\n",
       "      <td>71.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.19</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00-05:00</td>\n",
       "      <td>18.93</td>\n",
       "      <td>21.84</td>\n",
       "      <td>25.47</td>\n",
       "      <td>68.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.53</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00-05:00</td>\n",
       "      <td>18.63</td>\n",
       "      <td>22.78</td>\n",
       "      <td>26.17</td>\n",
       "      <td>76.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00-05:00</td>\n",
       "      <td>20.37</td>\n",
       "      <td>23.84</td>\n",
       "      <td>26.55</td>\n",
       "      <td>63.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00-05:00</td>\n",
       "      <td>17.39</td>\n",
       "      <td>21.74</td>\n",
       "      <td>25.87</td>\n",
       "      <td>72.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  tmin_C  tmean_C  tmax_C  rh_pct  precip_mm  \\\n",
       "0 2020-01-01 00:00:00-05:00   16.98    22.60   26.05    71.3        5.0   \n",
       "1 2020-01-02 00:00:00-05:00   18.93    21.84   25.47    68.8        2.0   \n",
       "2 2020-01-03 00:00:00-05:00   18.63    22.78   26.17    76.3        6.0   \n",
       "3 2020-01-04 00:00:00-05:00   20.37    23.84   26.55    63.9        3.0   \n",
       "4 2020-01-05 00:00:00-05:00   17.39    21.74   25.87    72.8        6.0   \n",
       "\n",
       "   wind_m_s          station_id  \n",
       "0      2.19  IDEAM_Caucasia_001  \n",
       "1      4.53  IDEAM_Caucasia_001  \n",
       "2      3.33  IDEAM_Caucasia_001  \n",
       "3      1.34  IDEAM_Caucasia_001  \n",
       "4      3.15  IDEAM_Caucasia_001  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4a5f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC-05:00]",
         "type": "unknown"
        },
        {
         "name": "tmin_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmean_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmax_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rh_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_m_s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "eb6b53b9-263a-4404-8e92-86e04963362f",
       "rows": [
        [
         "0",
         "2020-01-01 00:00:00-05:00",
         "16.72",
         "20.84",
         "23.12",
         "83.1",
         "4.0",
         "2.01",
         "MiraCielo_002"
        ],
        [
         "1",
         "2020-01-02 00:00:00-05:00",
         "16.52",
         "21.19",
         "24.51",
         "75.9",
         "5.0",
         "5.43",
         "MiraCielo_002"
        ],
        [
         "2",
         "2020-01-03 00:00:00-05:00",
         "20.71",
         "23.86",
         "28.03",
         "74.8",
         "4.0",
         "1.58",
         "MiraCielo_002"
        ],
        [
         "3",
         "2020-01-04 00:00:00-05:00",
         "18.06",
         "22.55",
         "26.46",
         "78.7",
         "3.0",
         "2.41",
         "MiraCielo_002"
        ],
        [
         "4",
         "2020-01-05 00:00:00-05:00",
         "17.24",
         "21.98",
         "26.65",
         "83.1",
         "2.0",
         "4.83",
         "MiraCielo_002"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tmin_C</th>\n",
       "      <th>tmean_C</th>\n",
       "      <th>tmax_C</th>\n",
       "      <th>rh_pct</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>wind_m_s</th>\n",
       "      <th>station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00-05:00</td>\n",
       "      <td>16.72</td>\n",
       "      <td>20.84</td>\n",
       "      <td>23.12</td>\n",
       "      <td>83.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00-05:00</td>\n",
       "      <td>16.52</td>\n",
       "      <td>21.19</td>\n",
       "      <td>24.51</td>\n",
       "      <td>75.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.43</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00-05:00</td>\n",
       "      <td>20.71</td>\n",
       "      <td>23.86</td>\n",
       "      <td>28.03</td>\n",
       "      <td>74.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00-05:00</td>\n",
       "      <td>18.06</td>\n",
       "      <td>22.55</td>\n",
       "      <td>26.46</td>\n",
       "      <td>78.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00-05:00</td>\n",
       "      <td>17.24</td>\n",
       "      <td>21.98</td>\n",
       "      <td>26.65</td>\n",
       "      <td>83.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.83</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  tmin_C  tmean_C  tmax_C  rh_pct  precip_mm  \\\n",
       "0 2020-01-01 00:00:00-05:00   16.72    20.84   23.12    83.1        4.0   \n",
       "1 2020-01-02 00:00:00-05:00   16.52    21.19   24.51    75.9        5.0   \n",
       "2 2020-01-03 00:00:00-05:00   20.71    23.86   28.03    74.8        4.0   \n",
       "3 2020-01-04 00:00:00-05:00   18.06    22.55   26.46    78.7        3.0   \n",
       "4 2020-01-05 00:00:00-05:00   17.24    21.98   26.65    83.1        2.0   \n",
       "\n",
       "   wind_m_s     station_id  \n",
       "0      2.01  MiraCielo_002  \n",
       "1      5.43  MiraCielo_002  \n",
       "2      1.58  MiraCielo_002  \n",
       "3      2.41  MiraCielo_002  \n",
       "4      4.83  MiraCielo_002  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amsc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a355a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 2. Cargar datos epidemiol√≥gicos\n",
    "# -------------------------------\n",
    "epi = pd.read_csv(CARPETA+\"\\epidemiology_caucasia_municipal.csv\", parse_dates=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec018c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC-05:00]",
         "type": "unknown"
        },
        {
         "name": "cases",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hospitalizations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "population",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "51472304-700e-4784-9c18-82eba87ecb91",
       "rows": [
        [
         "0",
         "2020-01-01 00:00:00-05:00",
         "6.0",
         "0",
         "140000"
        ],
        [
         "1",
         "2020-01-02 00:00:00-05:00",
         "4.0",
         "0",
         "140000"
        ],
        [
         "2",
         "2020-01-03 00:00:00-05:00",
         "5.0",
         "0",
         "140000"
        ],
        [
         "3",
         "2020-01-04 00:00:00-05:00",
         "6.0",
         "1",
         "140000"
        ],
        [
         "4",
         "2020-01-05 00:00:00-05:00",
         "9.0",
         "0",
         "140000"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cases</th>\n",
       "      <th>hospitalizations</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00-05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00-05:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00-05:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00-05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00-05:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  cases  hospitalizations  population\n",
       "0 2020-01-01 00:00:00-05:00    6.0                 0      140000\n",
       "1 2020-01-02 00:00:00-05:00    4.0                 0      140000\n",
       "2 2020-01-03 00:00:00-05:00    5.0                 0      140000\n",
       "3 2020-01-04 00:00:00-05:00    6.0                 1      140000\n",
       "4 2020-01-05 00:00:00-05:00    9.0                 0      140000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 3. Resumen de los datasets\n",
    "# -------------------------------\n",
    "print(\"IDEAM head:\")\n",
    "display(ideam.head())\n",
    "\n",
    "print(\"Mira Cielo head:\")\n",
    "display(mira_cielo.head())\n",
    "\n",
    "print(\"Epidemiolog√≠a head:\")\n",
    "display(epi.head())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Gr√°ficos meteorol√≥gicos\n",
    "# -------------------------------\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "sns.lineplot(data=ideam, x=\"date\", y=\"temperature\", ax=axes[0], label=\"Temperatura IDEAM\")\n",
    "axes[0].set_ylabel(\"¬∞C\")\n",
    "\n",
    "sns.lineplot(data=ideam, x=\"date\", y=\"humidity\", ax=axes[1], label=\"Humedad IDEAM\")\n",
    "axes[1].set_ylabel(\"%\")\n",
    "\n",
    "sns.lineplot(data=ideam, x=\"date\", y=\"precipitation\", ax=axes[2], label=\"Precipitaci√≥n IDEAM\")\n",
    "axes[2].set_ylabel(\"mm\")\n",
    "\n",
    "fig.suptitle(\"Datos Meteorol√≥gicos IDEAM - Caucasia (2020‚Äì2025)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Gr√°ficos epidemiol√≥gicos\n",
    "# -------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=epi, x=\"date\", y=\"cases\", ax=ax, label=\"Casos diarios\")\n",
    "sns.lineplot(data=epi, x=\"date\", y=\"hospitalizations\", ax=ax, label=\"Hospitalizaciones\")\n",
    "ax.set_ylabel(\"N√∫mero de casos\")\n",
    "ax.set_title(\"Epidemiolog√≠a - Caucasia (2020‚Äì2025)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Res√∫menes agregados\n",
    "# -------------------------------\n",
    "# Resumen semanal\n",
    "epi_weekly = epi.resample(\"W\", on=\"date\").sum(numeric_only=True)\n",
    "print(\"Resumen epidemiol√≥gico semanal:\")\n",
    "display(epi_weekly.head())\n",
    "\n",
    "# Resumen mensual de IDEAM\n",
    "ideam_monthly = ideam.resample(\"M\", on=\"date\").mean(numeric_only=True)\n",
    "print(\"Resumen meteorol√≥gico mensual (IDEAM):\")\n",
    "display(ideam_monthly.head())\n",
    "\n",
    "# Visualizaci√≥n resumen mensual\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=ideam_monthly, x=ideam_monthly.index, y=\"temperature\", ax=ax, label=\"Temperatura media\")\n",
    "sns.lineplot(data=ideam_monthly, x=ideam_monthly.index, y=\"humidity\", ax=ax, label=\"Humedad media\")\n",
    "ax.set_ylabel(\"Valores promedio\")\n",
    "ax.set_title(\"Resumen mensual Meteorol√≥gico IDEAM\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f814d",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "üìå **C√≥mo usarlo en VS Code (Jupyter):**\n",
    "\n",
    "1. Coloca el script en una celda de tu cuaderno `.ipynb`.\n",
    "2. Aseg√∫rate de que los `.csv` est√©n en el mismo directorio del cuaderno (o cambia las rutas en `pd.read_csv`).\n",
    "3. Ejecuta celda por celda para ver las tablas y gr√°ficos.\n",
    "\n",
    "---\n",
    "\n",
    "¬øQuieres que te adapte este mismo script para que genere un **dashboard interactivo** con `ipywidgets` (sliders para elegir rango de fechas, variables, etc.)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae770ed5",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136505ef",
   "metadata": {},
   "source": [
    "\n",
    "# 2) Qu√© se genera (archivos)\n",
    "\n",
    "Al ejecutar el script que yo corr√≠ aqu√≠, el paquete generado contiene:\n",
    "\n",
    "* `meteorology_IDEAM_Caucasia_001.csv` ‚Äî serie diaria 2020-01-01 ‚Üí 2025-12-31 con tmin, tmean, tmax, rh\\_pct, precip\\_mm, wind\\_m\\_s y `station_id`.\n",
    "* `meteorology_MiraCielo_002.csv` ‚Äî segunda estaci√≥n con un hueco declarado (sensor change) en 2021-11-02 ‚Üí 2021-11-12 (NaNs introducidos intencionalmente).\n",
    "* `epidemiology_caucasia_municipal.csv` ‚Äî casos diarios y hospitalizaciones; incluye hueco en 2022-08-10 ‚Üí 2022-08-17 para emular fallos en reporte.\n",
    "* `stations_metadata.csv` ‚Äî metadatos de estaciones (lat/lon/elev/source/notes).\n",
    "* `inventory.json` ‚Äî inventario con SHA256 para cada CSV, periodo, frecuencia y metadatos de procedencia.\n",
    "* `caucasia_synthetic_raw_2020_2025.zip` ‚Äî ZIP con todos los archivos anteriores.\n",
    "\n",
    "En el entorno donde ejecut√© el script (notebook), el ZIP qued√≥ en:\n",
    "`/mnt/data/caucasia_synthetic_raw_2020_2025.zip`\n",
    "\n",
    "Y los archivos CSV en:\n",
    "`/mnt/data/raw/`\n",
    "\n",
    "Hashes mostrados (ejemplo generado):\n",
    "\n",
    "* meteorology\\_IDEAM\\_Caucasia\\_001.csv: `c49261e9...`\n",
    "* meteorology\\_MiraCielo\\_002.csv: `316d33b5...`\n",
    "* epidemiology\\_caucasia\\_municipal.csv: `97db3db1...`\n",
    "* stations\\_metadata.csv: `d16ce287...`\n",
    "\n",
    "(En ejecuci√≥n local tus hashes cambiar√°n si replicas el proceso con distinta semilla.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8aa88",
   "metadata": {},
   "source": [
    "\n",
    "# 3) C√≥mo usarlo en tu pipeline ETL\n",
    "\n",
    "Recomendaciones r√°pidas:\n",
    "\n",
    "* Integra este script como una etapa `01_generate_synthetic.py` dentro de `/data/raw/` para pruebas del pipeline.\n",
    "* Para pasar a la fase de limpieza (`02_clean.py`) apunta a `raw/*.csv` y usa `inventory.json` para validar integridad (hashes y periodos).\n",
    "* Mant√©n la semilla (`np.random.seed(...)`) si quieres reproducibilidad exacta entre ejecuciones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aac22e",
   "metadata": {},
   "source": [
    "\n",
    "# 4) ¬øQuieres que haga algo m√°s ahora?\n",
    "\n",
    "Puedo, seg√∫n prefieras:\n",
    "\n",
    "* Ajustar la generaci√≥n (m√°s estaciones, variabilidad espacial, diferentes poblaciones municipales).\n",
    "* Incluir variables adicionales (radiaci√≥n solar, velocidad del viento horaria, √≠ndice de vegetaci√≥n).\n",
    "* Entregar el notebook ETL completo (descarga ‚Üí limpieza ‚Üí transformaciones ‚Üí integraci√≥n) listo para correr con estos archivos sint√©ticos.\n",
    "* Generar un conjunto de pruebas de calidad autom√°ticas (checks) que validen rangos, continuidad, y alertas en `inventory.json`.\n",
    "\n",
    "Dime cu√°l de estas opciones quieres y lo hago ahora mismo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccac8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a3f5b2a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Limpieza de Datos\n",
    "\n",
    "**Objetivo**: detectar y corregir faltantes, errores de medici√≥n y valores at√≠picos sin sesgar la se√±al temporal.\n",
    "\n",
    "**Pasos y t√©cnicas**\n",
    "\n",
    "1. **Verificaci√≥n inicial**\n",
    "\n",
    "   * Comprobar integridad de fechas (continuidad diaria), duplicados, y formatos.\n",
    "   * Identificar cambios en la estaci√≥n (cambios de sensor/ubicaci√≥n) usando metadatos.\n",
    "\n",
    "2. **Identificaci√≥n de faltantes y anomal√≠as**\n",
    "\n",
    "   * Estad√≠sticas por variable: % de datos faltantes por estaci√≥n/periodo.\n",
    "   * Visual: heatmap de presencia/ausencia por fecha.\n",
    "   * Identificar outliers con m√©todos combinados: IQR por ventana estacional, Z-score por temporada, y t√©cnicas robustas como **Hampel filter** y **Isolation Forest** para outliers multivariados (meteorolog√≠a + epidemiolog√≠a).\n",
    "\n",
    "3. **Manejo de datos faltantes**\n",
    "\n",
    "   * **Peque√±os huecos (1‚Äì3 d√≠as)**: interpolaci√≥n temporal (lineal o spline) o **interpolaci√≥n estacional** (por d√≠a del a√±o promedio).\n",
    "   * **Huecos medianos (4‚Äì30 d√≠as)**: imputaci√≥n por **modelo de series temporales** (ej. ETS o Kalman smoothing / state-space), o por regresi√≥n con estaciones cercanas (spatial kriging o regresi√≥n m√∫ltiple).\n",
    "   * **Huecos largos (>30 d√≠as)**: marcar como no imputados si la imputaci√≥n es arriesgada; usar variables agregadas (semana/mes) donde sea viable.\n",
    "   * **Epidemiolog√≠a ‚Äì casos**: evitar imputaciones autom√°ticas agresivas. Cuando falten reportes, documentar cambios de sistema de reporte; considerar imputaci√≥n por distribuci√≥n de Poisson/Gamma condicionada a tendencias de semanas previas y covariables meteorol√≥gicas s√≥lo si est√° justificado.\n",
    "\n",
    "4. **Correcci√≥n de errores de medici√≥n**\n",
    "\n",
    "   * Validar rangos f√≠sicos (p. ej. temperatura plausible, humedad 0‚Äì100%). Valores fuera de rango ‚Üí marcar como inv√°lidos.\n",
    "   * Si hay saltos bruscos coincidentes con cambios de sensor, ajustar series por comparaciones con estaciones vecinas usando regresi√≥n para calibrar.\n",
    "\n",
    "5. **Registro de decisiones**\n",
    "\n",
    "   * Crear un log de limpieza (tipo) por fecha y registro (qu√© se imput√≥, m√©todo, justificativo).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Transformaci√≥n de Datos\n",
    "\n",
    "**Objetivo**: preparar variables adecuadas para an√°lisis y para ser usadas como ex√≥genas en SARIMAX.\n",
    "\n",
    "**Transformaciones recomendadas**\n",
    "\n",
    "1. **Fechas y calendario**\n",
    "\n",
    "   * Convertir a √≠ndice de tiempo (DatetimeIndex) en pandas; crear columnas: `date`, `year`, `month`, `day`, `dow` (d√≠a semana), `doy` (d√≠a del a√±o), `epiweek` (semana epidemiol√≥gica ISO) si usar√°s an√°lisis semanal.\n",
    "\n",
    "2. **Variables derivadas meteorol√≥gicas**\n",
    "\n",
    "   * **Medias m√≥viles**: 7, 14, 30 d√≠as (ej.: `temp_7d_mean`, `precip_14d_sum`).\n",
    "   * **Acumulados**: precipitaci√≥n acumulada 7/14/30 d√≠as.\n",
    "   * **√çndices**: *heat index* (temperatura + humedad), *evapotranspiraci√≥n aproximada* (si tienes radiaci√≥n o viento), `Tmin`, `Tmax` diarias.\n",
    "   * **Anomal√≠as estacionales**: valor ‚àí media diurna hist√≥rica (por d√≠a del a√±o), para resaltar episodios an√≥malos.\n",
    "   * **Indicadores discretos**: `had_precip` (booleano si lluvia > 0.1 mm).\n",
    "\n",
    "3. **Variables epidemiol√≥gicas**\n",
    "\n",
    "   * **Counts vs rates**: preferir tasas por 1000 habitantes cuando sea relevante; si solo tienes counts, incluir `population` para convertir.\n",
    "   * **Transformaciones**: `log(count + 1)` para estabilizar varianza; diferencia (diff) si modelas cambios.\n",
    "   * **Lags**: crear `cases_lag_1..lag_k` y lags meteorol√≥gicos (`precip_lag_0..lag_m`) hasta el m√°ximo plausible (p. ej. 0‚Äì28 d√≠as) seg√∫n biolog√≠a del vector/enfermedad.\n",
    "   * **Promedios lagged**: media de temperatura lag 7‚Äì14 d√≠as (para efectos retrasados).\n",
    "\n",
    "4. **Escalado/normalizaci√≥n**\n",
    "\n",
    "   * Para selecci√≥n de covariables y regularizaci√≥n: **standard scaler** (z-score) o **min-max**; mantener copias sin escalar para interpretabilidad de coeficientes.\n",
    "   * No escalar variables que vayas a usar en transformaciones dirigidas a SARIMAX si quieres interpretar magnitudes directamente; escalar cuando uses algoritmos de selecci√≥n multivariada o regularizados.\n",
    "\n",
    "5. **Estacionariedad y transformaciones de Box‚ÄìCox**\n",
    "\n",
    "   * Probar **ADF** para stationarity; si no estacionaria, usar **diferenciaci√≥n** (d, D) y/o Box‚ÄìCox (registrar lambda).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Integraci√≥n de Datos\n",
    "\n",
    "**Objetivo**: producir un dataset final con √≠ndice temporal consistente y variables debidamente alineadas para SARIMAX (end√≥gena + ex√≥genas).\n",
    "\n",
    "**Pasos concretos**\n",
    "\n",
    "1. **Unificar frecuencia**\n",
    "\n",
    "   * Mantener **diaria**; si algunas series son horarias, agregarlas (media/suma) a diario. Si epidemiolog√≠a solo est√° semanal, mantener diaria con NaN diarios y luego agregar (o expandir semanal a diario con la misma tasa ‚Äî documentar). Para SARIMAX es preferible tener la misma frecuencia en end√≥gena y ex√≥genas.\n",
    "\n",
    "2. **Alineamiento temporal**\n",
    "\n",
    "   * **Join** por fecha (left-join usando la serie end√≥gena como referencia).\n",
    "   * Para datos con frecuencia distinta: `resample('D').sum()` o `.mean()` seg√∫n contexto. Para pasar de diaria a semanal usar `resample('W-MON')` o `W-SUN` seg√∫n definici√≥n de semana epidemiol√≥gica.\n",
    "\n",
    "3. **Manejo de desfases biol√≥gicos**\n",
    "\n",
    "   * Construir una **matriz de dise√±o** de ex√≥genas con m√∫ltiples lags y agregados (p. ej. `precip_lag_7`, `temp_14d_mean`) para explorar retardos en EDA y en selecci√≥n de modelo.\n",
    "   * Procurar no incluir lags redundantes que generen multicolinealidad extrema; usar selecci√≥n por VIF o PCA cuando sea necesario.\n",
    "\n",
    "4. **Coherencia y validaci√≥n**\n",
    "\n",
    "   * Verificar que despu√©s de la integraci√≥n no hayan saltos no explicados (p. ej. cuando falt√≥ toda una semana en meteorolog√≠a).\n",
    "   * Comprobar correlaciones temporales y que las fechas de eventos (picos epid√©micos) coincidan con la informaci√≥n de notificaci√≥n.\n",
    "\n",
    "5. **Formato de salida**\n",
    "\n",
    "   * Guardar dataset consolidado en **Parquet** (compacto, mantiene tipos) y en **CSV** para interoperabilidad.\n",
    "   * Acompa√±ar con `metadata.yml` que describa variables, unidades, m√©todo de imputaci√≥n y versi√≥n.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) An√°lisis Exploratorio de Datos (EDA)\n",
    "\n",
    "**Objetivo**: entender patrones estacionales, tendencias, relaciones y retardos entre meteorolog√≠a y epidemiolog√≠a antes de modelar.\n",
    "\n",
    "**T√©cnicas y visualizaciones recomendadas**\n",
    "\n",
    "1. **Series temporales**\n",
    "\n",
    "   * Gr√°ficas de la serie end√≥gena (casos, hospitalizaciones) superpuestas con principales ex√≥genas (temp, precip) ‚Äî plot diario y smoothing (LOESS).\n",
    "   * Gr√°ficas desagregadas por a√±o para visualizar estacionalidad.\n",
    "\n",
    "2. **Descomposici√≥n**\n",
    "\n",
    "   * `seasonal_decompose` (additive/multiplicative) o STL para separar tendencia, estacionalidad y residuo.\n",
    "\n",
    "3. **Autocorrelaciones**\n",
    "\n",
    "   * ACF/PACF de la serie end√≥gena y de los residuos iniciales (para sugerir √≥rdenes ARIMA p,q y diferencias).\n",
    "   * CCF (cross-correlation function) entre casos y cada meteorol√≥gica para detectar retardos con m√°xima correlaci√≥n (ej.: precip lag 10).\n",
    "\n",
    "4. **Correlaciones y lag plots**\n",
    "\n",
    "   * Heatmap de correlaci√≥n entre variables y entre lags (matriz corr de `cases` vs `temp_lag_0..28`, etc.).\n",
    "   * Scatter plots y *lag scatter plots* para patrones no lineales.\n",
    "\n",
    "5. **Estad√≠sticas descriptivas**\n",
    "\n",
    "   * Media, mediana, desviaci√≥n, percentiles, % de ceros (precip), tasas por 1000 hab.\n",
    "   * Distribuciones (histogramas) y QQ-plots para verificar normalidad (√∫til para decidir transformaciones).\n",
    "\n",
    "6. **Identificaci√≥n de rupturas estructurales**\n",
    "\n",
    "   * Pruebas de cambio de r√©gimen (CUSUM, Chow test) si fue implementado un cambio en reportes o intervenci√≥n sanitaria.\n",
    "\n",
    "7. **An√°lisis de colinealidad**\n",
    "\n",
    "   * VIF para ex√≥genas; si VIF alto (>10) considerar combinar variables (PCA) o eliminar.\n",
    "\n",
    "8. **Pruebas causales exploratorias**\n",
    "\n",
    "   * **Granger causality tests** para ver si meteorolog√≠a ‚Äúgranger-predice‚Äù casos (no es prueba causal definitiva, pero orienta lags).\n",
    "\n",
    "9. **Visuales recomendados** (resumen)\n",
    "\n",
    "   * Time series plots (end√≥gena + ex√≥genas con ejes doble).\n",
    "   * STL decomposition.\n",
    "   * ACF/PACF y CCF.\n",
    "   * Heatmap de correlaci√≥n lags.\n",
    "   * Boxplots por mes/estaci√≥n (para ver estacionalidad).\n",
    "   * Mapas (si hay variaci√≥n espacial entre estaciones).\n",
    "\n",
    "10. **Selecci√≥n preliminar de covariables**\n",
    "\n",
    "* Basada en: fuerza de correlaci√≥n en los lags biol√≥gicamente plausibles, significado epidemiol√≥gico, VIF y parsimonia.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Documentaci√≥n y Reproducibilidad\n",
    "\n",
    "**Objetivo**: asegurar que cualquier investigador pueda reproducir la extracci√≥n, limpieza y consolidaci√≥n, y que el pipeline sea auditable y versionable.\n",
    "\n",
    "**Pr√°cticas obligatorias**\n",
    "\n",
    "1. **Control de versiones**\n",
    "\n",
    "   * Repositorio Git (GitHub/GitLab) con ramas `main`, `dev`.\n",
    "   * Commits claros; usar *issues* y *pull requests* para cambios mayores.\n",
    "\n",
    "2. **Pipeline reproducible**\n",
    "\n",
    "   * Organizar en carpetas: `/data/raw`, `/data/processed`, `/notebooks`, `/src`, `/docs`.\n",
    "   * Scripts ETL en `/src` (por ejemplo `01_fetch.py`, `02_clean.py`, `03_transform.py`, `04_integrate.py`) ejecutables en orden.\n",
    "   * Uso de **Makefile** o **Snakemake** para orquestar pasos y dependencias.\n",
    "\n",
    "3. **Entorno**\n",
    "\n",
    "   * `environment.yml` (conda) o `requirements.txt` (pip) y `Dockerfile` para reproducibilidad de entorno.\n",
    "   * Versionar librer√≠as cr√≠ticas (pandas, statsmodels, scikit-learn).\n",
    "\n",
    "4. **Notebooks y pruebas**\n",
    "\n",
    "   * Notebooks Jupyter para EDA y reportes; scripts para ETL.\n",
    "   * Tests unitarios simples para funciones cr√≠ticas (por ejemplo: test de integridad de fechas, test de no duplicados), en `tests/`.\n",
    "\n",
    "5. **Metadatos y registros**\n",
    "\n",
    "   * `metadata.yml` explicando variables, unidades, origen, m√©todo de imputation y cambios de versi√≥n de datos.\n",
    "   * `provenance.log` o `data_catalog.csv` con hashes de archivos originales, fecha extracci√≥n, y persona responsable.\n",
    "\n",
    "6. **Documentos y reportes**\n",
    "\n",
    "   * Generar informes reproducibles (nbconvert, papermill, or GitHub Actions) que creen reportes PDF/HTML con EDA y resumen de calidad.\n",
    "   * Mantener un changelog (CHANGELOG.md) para la base de datos y los modelos.\n",
    "\n",
    "7. **Pol√≠ticas y √©tica**\n",
    "\n",
    "   * Si datos epidemiol√≥gicos son sensibles: anonimizar, agregar por fecha/√°rea m√≠nima, revisar permisos y pol√≠ticas de protecci√≥n de datos (Ley de Protecci√≥n de Datos en Colombia).\n",
    "   * Guardar y documentar consentimiento/uso cuando aplique.\n",
    "\n",
    "8. **Entrega y preservaci√≥n**\n",
    "\n",
    "   * Versi√≥n final del dataset (por ejemplo: `caucasia_timeseries_v1.0.parquet`) con DOI si se publica (Zenodo / institutional repo).\n",
    "   * Backup regular y control de acceso (roles).\n",
    "\n",
    "---\n",
    "\n",
    "## Ap√©ndice pr√°ctico (resumen t√©cnico y recomendaciones concretas)\n",
    "\n",
    "* **Frecuencia**: conservar diaria, agregar semanal/mes seg√∫n necesidad.\n",
    "* **Periodo de an√°lisis**: objetivo ‚â• 10‚Äì16 a√±os (2007‚Äì2024 recomendado).\n",
    "* **Imputaci√≥n preferida**: peque√±a‚Äîinterpolaci√≥n; mediana‚ÄîKalman smoothing o model-based; grande‚Äîusar datos de estaciones vecinas o marcar como no confiable.\n",
    "* **Outliers**: combinar filtros estad√≠sticos (IQR/Hampel) y t√©cnicas de ML (Isolation Forest) para detecci√≥n; manual review cuando afecte picos epid√©micos.\n",
    "* **Formato de almacenamiento**: Parquet + metadata YAML; raw en CSV/zip.\n",
    "* **Pipeline**: scripts versionados + Docker + pruebas automatizadas.\n",
    "* **Validaci√≥n de modelos**: usar validaci√≥n temporal (rolling-origin / expanding window) y m√©tricas RMSE/MAE/MAPE; revisar residuos (no autocorrelaci√≥n, homocedasticidad).\n",
    "* **Selecci√≥n ex√≥genas para SARIMAX**: basarse en EDA (CCF + plausibilidad biol√≥gica); controlar multicolinealidad; probar modelos con distintos sets de lags y seleccionar por AIC/BIC y performance out-of-sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Consejos operativos finales y decisiones pr√°cticas\n",
    "\n",
    "1. **Empieza por un ‚Äúproyecto piloto‚Äù** con 3‚Äì5 a√±os de datos completos para validar pipeline y decisiones de imputaci√≥n antes de aplicar al archivo hist√≥rico completo.\n",
    "2. **Documenta cada decisi√≥n** en el `provenance.log` (p. ej. ‚Äúimput√© 2012-03-05..2012-03-08 precip con spline debido falla de sensor; fuente estaci√≥n X‚Äù).\n",
    "3. **Mant√©n comunicaci√≥n activa con el personal local** (Secretar√≠a de Salud y t√©cnicos IDEAM) para conocer cambios de protocolo y mantenimiento de estaciones.\n",
    "4. **Automatiza** extracciones peri√≥dicas y notificaciones de calidad (p. ej. alerta si % faltantes diario > 5%).\n",
    "\n",
    "---\n",
    "\n",
    "Si quieres, te puedo entregar lo siguiente **listo para ejecutar** (elige uno o varios):\n",
    "\n",
    "* Un **notebook Jupyter** (Python/pandas/statsmodels) con un pipeline ETL m√≠nimo (descarga-sint√©tica ‚Üí limpieza ‚Üí transformaciones ‚Üí dataset final) y ejemplos de EDA y tests ADF/ACF para SARIMAX.\n",
    "* Plantillas `metadata.yml`, `provenance.log` y `Makefile`/`Dockerfile` para producci√≥n.\n",
    "* Un esquema de **matriz de lags** sugerida (precip/temp/humidity con lags 0‚Äì28 y medias m√≥viles 7/14/30) listo para usar.\n",
    "\n",
    "¬øQuieres que te genere el notebook ETL ejemplo y las plantillas comentadas para tu proyecto en Caucasia (lo creo en formato Jupyter listo para ejecutar)? Puedo prepararlo inmediatamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651548f",
   "metadata": {},
   "source": [
    "# Metodolog√≠a para tratamiento de los datos en esta investigaci√≥n \n",
    "\n",
    "1. Generacion los datos meteorol√≥gicos y epidemiol√≥gicos que emulen los de [AMSC](https://www.antioquiamirasucielo.org/): \n",
    "2. Generacion los datos meteorol√≥gicos y epidemiol√≥gicos que emulen los de NASA POWER:   \n",
    "3. determinar los rezagos de cada variable. Es decir, los $k_{i}$ de cada variable $X_{i}$.\n",
    "4. Transformar los datos en datos que tienen en cuenta tales rezagos. Es decir, que las variables ya queden de la forma $X_{i, t-k_{i}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a46cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
