{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "255d6971",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/algebra_y_trigonometria/blob/main/classes/0_formatos_clase/algebra_and_trigonometry.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "  <td>\n",
    "    <a target=\"_blank\" href=\"https://kaggle.com/kernels/welcome?src=https://github.com/marco-canas/algebra_y_trigonometria/blob/main/classes/0_formatos_clase/algebra_and_trigonometry.ipynb\"><img src=\"https://kaggle.com/static/images/open-in-kaggle.svg\" /></a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0669f881",
   "metadata": {},
   "source": [
    "# Prompt para el diseño de la metodología de consolidación de datos meteorológicos y epidemiológicos: \n",
    "\n",
    "Emulando a un experto en análisis de series temporales y modelado estadístico, especializado en la consolidación de datos meteorológicos y epidemiológicos. Diseña una metodología detallada para consolidar estos datos meteorológicos y epidemiológicos de Caucasia Antioquia (Colombia) para ser modelados con SARIMAX, asegurando su calidad y relevancia para futuros análisis. La metodología debe incluir los siguientes pasos:   \n",
    "\n",
    "\n",
    "1. **Recolección de Datos**: Describe cómo y dónde se obtendrán los datos meteorológicos (por ejemplo, temperatura, humedad, precipitaciones) y epidemiológicos (por ejemplo, tasas de infección, hospitalizaciones) de fuentes confiables. Incluye consideraciones sobre la frecuencia de los datos (diaria, semanal, mensual) y el período de tiempo a cubrir.  \n",
    "2. **Limpieza de Datos**: Explica los procedimientos para identificar y manejar datos faltantes, valores atípicos y errores de medición. Detalla las técnicas que se utilizarán para imputar datos faltantes y corregir errores, asegurando la integridad de los conjuntos de datos.  \n",
    "3. **Transformación de Datos**: Describe cómo se transformarán los datos para facilitar su análisis. Esto puede incluir la normalización, estandarización, creación de variables derivadas (por ejemplo, medias móviles, índices de calor) y la conversión de formatos de fecha y hora.  \n",
    "4. **Integración de Datos**: Explica cómo se combinarán los datos meteorológicos y epidemiológicos en un solo conjunto de datos. Detalla los métodos para alinear temporalmente los datos, manejar diferentes frecuencias de muestreo y asegurar la coherencia entre las variables.  \n",
    "5. **Análisis Exploratorio de Datos (EDA)**: Describe las técnicas que se utilizarán para explorar los datos consolidados, identificar patrones, tendencias y relaciones entre las variables meteorológicas y epidemiológicas. Incluye visualizaciones recomendadas y estadísticas descriptivas.  \n",
    "6. **Documentación y Reproducibilidad**: Explica cómo se documentará cada paso del proceso de consolidación de datos para asegurar la reproducibilidad. Incluye recomendaciones sobre el uso de herramientas de control de versiones y la creación de informes detallados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97276c7c",
   "metadata": {},
   "source": [
    "# Metodología hasta hoy 21 de Septiembre de 2025  \n",
    "\n",
    "Esta es una metodología práctica, detallada y reproducible (emulando a un experto en series temporales y modelado SARIMAX) para consolidar datos meteorológicos y epidemiológicos de **Caucasia, Antioquia (Colombia)**, lista para ejecutar y reutilizar por tu equipo de investigación. Incluyo recomendaciones técnicas, criterios de calidad y pasos concretos para cada etapa solicitada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d352ed",
   "metadata": {},
   "source": [
    "\n",
    "# Metodología para consolidación de datos meteorológicos y epidemiológicos (orientada a SARIMAX)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5260f92",
   "metadata": {},
   "source": [
    "\n",
    "## 1) Recolección de Datos\n",
    "\n",
    "**Objetivo**: reunir series temporales completas y bien documentadas de variables meteorológicas y epidemiológicas.\n",
    "\n",
    "**Fuentes recomendadas (ejemplos prácticos para Caucasia)**\n",
    "\n",
    "* **Meteorología**: IDEAM (estaciones meteorológicas nacionales), proyectos regionales como *Antioquia — Mira su Cielo*, repositorios académicos o redes agrícolas (p. ej. datos de Fedearroz si disponibles para lluvia/temperatura).\n",
    "* **Epidemiología**: Secretaría de Salud Municipal de Caucasia (casos notificados, hospitalizaciones), Secretaría Seccional de Salud de Antioquia, INS (boletines nacionales), registros hospitalarios locales (EPS/hospital), SIVIGILA (cuando aplique).\n",
    "* **Metadatos y geolocalización**: fichas de estación IDEAM (altura, lat/long, cambios de sensor), documentación de protocolos de muestreo epidemiológico (definición de caso, fecha de inicio, fecha de notificación).\n",
    "\n",
    "**Frecuencia y período recomendado**\n",
    "\n",
    "* **Frecuencia primaria**: **diaria** (ideal para meteorología y para modelar incidencia con retardo).\n",
    "* **Agrupaciones**: conservar la granularidad diaria y proporcionar resúmenes semanales/mensuales según necesidad.\n",
    "* **Período mínimo aconsejado**: **>10 años**; si es posible usar **2020–2025 (≥5 años)** para capturar variabilidad interanual y estacional — esto coincide con trabajos previos en la región y mejora la robustez del SARIMAX.\n",
    "* **Fecha y zona horaria**: registrar en **ISO 8601** (YYYY-MM-DD) y usar **America/Bogota (UTC-5)** consistentemente.\n",
    "\n",
    "**Práctica operativa**\n",
    "\n",
    "* Automatizar descargas (APIs, FTP, scrapers controlados) y almacenar **raw/** sin modificar.\n",
    "* Registrar el *hash* (SHA256) de cada archivo crudo y la fecha de extracción.\n",
    "* Mantener un inventario (CSV/JSON) con: fuente, variable, frecuencia, periodo, contacto, licencias.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1694de4",
   "metadata": {},
   "source": [
    "Listo — generé y guardé un script (ejecutado aquí) que crea datos artificiales diarios para **2020-01-01 → 2025-12-31** y produce un paquete \"raw\" con metadatos e inventario. Abajo te dejo:\n",
    "\n",
    "1. El **script completo** (para copiar/pegar y ejecutar localmente).\n",
    "2. Un resumen de lo que genera y los archivos de salida, incluyendo la ruta del ZIP con todos los CSV raw.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44bf822",
   "metadata": {},
   "source": [
    "\n",
    "# 1) Script (copia — es el mismo que ejecuté aquí)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f8266e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_3700\\1680677275.py:73: FutureWarning: Series.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  temp_vals = pd.read_csv(OUT_DIR / f\"meteorology_{stations[0]['station_id']}.csv\", parse_dates=[\"date\"])[\"tmean_C\"].fillna(method=\"ffill\").values\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generación completa. Archivos en: raw\n",
      "ZIP: caucasia_synthetic_raw_2020_2025.zip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generación de datos sintéticos para \"Recolección de Datos\" (Caucasia, Antioquia)\n",
    "# Crea datos meteorológicos diarios (2 estaciones) y datos epidemiológicos diarios\n",
    "# (casos y hospitalizaciones a nivel municipal) para 2020-01-01 a 2025-12-31.\n",
    "# Guarda archivos en ./raw/ y crea inventory.json con hashes SHA256.\n",
    "\n",
    "import os, json, hashlib, zipfile\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "np.random.seed(42)\n",
    "\n",
    "OUT_DIR = Path(\"raw\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "start_date = \"2020-01-01\"\n",
    "end_date = \"2025-12-31\"\n",
    "dates = pd.date_range(start=start_date, end=end_date, freq=\"D\", tz=\"America/Bogota\") # genera fechas diarias\n",
    "n = len(dates)\n",
    "\n",
    "def seasonal_temp(day_of_year, base=26.0, amp=6.0):\n",
    "    return base + amp * np.sin(2 * np.pi * (day_of_year / 365.25 - 0.25))\n",
    "\n",
    "def seasonal_precip(day_of_year, base=3.0, amp=3.0):\n",
    "    val = base + amp * np.cos(2 * np.pi * (day_of_year / 365.25 - 0.1))\n",
    "    return np.clip(val, 0.0, None)\n",
    "\n",
    "def sha256_of_file(path): # calcula SHA256 de un archivo \n",
    "    h = hashlib.sha256()\n",
    "    with open(path, \"rb\") as f:\n",
    "        for chunk in iter(lambda: f.read(8192), b\"\"):\n",
    "            h.update(chunk)\n",
    "    return h.hexdigest()\n",
    "\n",
    "stations = [\n",
    "    {\"station_id\": \"IDEAM_Caucasia_001\", \"name\": \"IDEAM Caucasia - Estación Principal\", \"lat\": 7.9320, \"lon\": -75.1980, \"elevation_m\": 45, \"source\": \"IDEAM\", \"notes\": \"Sensor automático. Calibración registrada 2022-06-15.\"},\n",
    "    {\"station_id\": \"MiraCielo_002\", \"name\": \"Antioquia - Mira su Cielo - Estación Caucasia\", \"lat\": 7.9350, \"lon\": -75.1925, \"elevation_m\": 48, \"source\": \"Antioquia - Mira su Cielo\", \"notes\": \"Proyecto regional. Cambio de sensor el 2021-11-02.\"}\n",
    "]\n",
    "\n",
    "# Generar meteorología por estación\n",
    "for st in stations:\n",
    "    day_of_year = dates.dayofyear.values\n",
    "    t_mean = seasonal_temp(day_of_year, base=26.5, amp=4.5)\n",
    "    noise = np.random.normal(scale=1.2, size=n)\n",
    "    t_mean = t_mean + noise + (0.01 * (dates.year - 2020))\n",
    "    t_min = t_mean - np.abs(np.random.normal(loc=4.0, scale=0.8, size=n))\n",
    "    t_max = t_mean + np.abs(np.random.normal(loc=4.0, scale=0.9, size=n))\n",
    "    humidity = np.clip(75 - 5 * np.sin(2 * np.pi * day_of_year/365.25) + np.random.normal(scale=6, size=n), 20, 100)\n",
    "    precip_base = seasonal_precip(day_of_year, base=3.5, amp=4.0)\n",
    "    precip = np.random.poisson(lam=np.clip(precip_base / 2.0, 0.01, None), size=n).astype(float)\n",
    "    heavy_idx = np.random.choice(n, size=int(0.02 * n), replace=False)\n",
    "    precip[heavy_idx] += np.random.gamma(shape=3.0, scale=10.0, size=len(heavy_idx))\n",
    "    wind = np.clip(np.random.normal(loc=2.8, scale=1.1, size=n), 0, None)\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"date\": dates,\n",
    "        \"tmin_C\": np.round(t_min, 2),\n",
    "        \"tmean_C\": np.round(t_mean, 2),\n",
    "        \"tmax_C\": np.round(t_max, 2),\n",
    "        \"rh_pct\": np.round(humidity, 1),\n",
    "        \"precip_mm\": np.round(precip, 2),\n",
    "        \"wind_m_s\": np.round(wind, 2),\n",
    "    })\n",
    "    df[\"station_id\"] = st[\"station_id\"]\n",
    "    if st[\"station_id\"].endswith(\"_002\"):\n",
    "        mask_gap = (df[\"date\"] >= \"2021-11-02\") & (df[\"date\"] <= \"2021-11-12\")\n",
    "        df.loc[mask_gap, [\"tmin_C\", \"tmean_C\", \"tmax_C\", \"rh_pct\", \"precip_mm\", \"wind_m_s\"]] = np.nan\n",
    "    df.to_csv(OUT_DIR / f\"meteorology_{st['station_id']}.csv\", index=False)\n",
    "\n",
    "# Generar epidemiología municipal (ligada a meteorología)\n",
    "precip_station0 = pd.read_csv(OUT_DIR / f\"meteorology_{stations[0]['station_id']}.csv\", parse_dates=[\"date\"])\n",
    "precip_vals = precip_station0[\"precip_mm\"].fillna(0).values\n",
    "temp_vals = pd.read_csv(OUT_DIR / f\"meteorology_{stations[0]['station_id']}.csv\", parse_dates=[\"date\"])[\"tmean_C\"].fillna(method=\"ffill\").values\n",
    "\n",
    "precip_14d = pd.Series(precip_vals).rolling(window=14, min_periods=1).sum().values\n",
    "temp_7d = pd.Series(temp_vals).rolling(window=7, min_periods=1).mean().values\n",
    "risk_index = (precip_14d / (np.percentile(precip_14d, 90)+1)) + ((temp_7d - np.mean(temp_7d)) / (np.std(temp_7d)+1))\n",
    "risk_index = (risk_index - np.nanmin(risk_index)) / (np.nanmax(risk_index) - np.nanmin(risk_index) + 1e-9)\n",
    "\n",
    "baseline = 5 + 3 * np.sin(2 * np.pi * (pd.to_datetime(dates).dayofyear / 365.25))\n",
    "seasonal_component = baseline + 8 * risk_index\n",
    "lam = np.clip(np.abs(np.array(seasonal_component, dtype=float)), 0.1, None)\n",
    "outbreak_days = np.random.choice(n, size=int(0.01 * n), replace=False)\n",
    "lam[outbreak_days] = lam[outbreak_days] + np.random.uniform(20, 80, size=len(outbreak_days))\n",
    "\n",
    "cases = np.random.poisson(lam=lam).astype(int)\n",
    "hosp_rate = np.clip(0.05 + 0.02 * (risk_index), 0.02, 0.25)\n",
    "hospitalizations = np.random.binomial(n=cases, p=hosp_rate).astype(int)\n",
    "\n",
    "df_epi = pd.DataFrame({\n",
    "    \"date\": dates,\n",
    "    \"cases\": cases,\n",
    "    \"hospitalizations\": hospitalizations,\n",
    "    \"population\": 140000\n",
    "})\n",
    "mask_missing = (df_epi[\"date\"] >= \"2022-08-10\") & (df_epi[\"date\"] <= \"2022-08-17\")\n",
    "df_epi.loc[mask_missing, \"cases\"] = np.nan\n",
    "df_epi.to_csv(OUT_DIR / \"epidemiology_caucasia_municipal.csv\", index=False)\n",
    "\n",
    "# Stations metadata and inventory\n",
    "meta_stations = pd.DataFrame(stations)\n",
    "meta_stations.to_csv(OUT_DIR / \"stations_metadata.csv\", index=False)\n",
    "\n",
    "inventory = []\n",
    "for p in list(OUT_DIR.glob(\"*.csv\")):\n",
    "    sha = sha256_of_file(p)\n",
    "    if \"meteorology\" in p.name:\n",
    "        var = \"meteorology\"\n",
    "    elif \"epidemiology\" in p.name:\n",
    "        var = \"epidemiology\"\n",
    "    else:\n",
    "        var = \"metadata\"\n",
    "    df_tmp = pd.read_csv(p, parse_dates=[\"date\"]) if \"date\" in pd.read_csv(p, nrows=1).columns else None\n",
    "    period_from, period_to = None, None\n",
    "    if df_tmp is not None and \"date\" in df_tmp.columns:\n",
    "        period_from = str(df_tmp[\"date\"].min())\n",
    "        period_to = str(df_tmp[\"date\"].max())\n",
    "    inventory.append({\n",
    "        \"file\": str(p),\n",
    "        \"variable\": var,\n",
    "        \"frequency\": \"daily\",\n",
    "        \"period_from\": period_from,\n",
    "        \"period_to\": period_to,\n",
    "        \"sha256\": sha,\n",
    "        \"source\": \"synthetic_simulation\",\n",
    "        \"contact\": \"Marco - ejemplo sintético\",\n",
    "        \"license\": \"CC-BY-4.0 (simulated)\"\n",
    "    })\n",
    "\n",
    "with open(OUT_DIR / \"inventory.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inventory, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# ZIP para distribución\n",
    "zip_path = Path(\"caucasia_synthetic_raw_2020_2025.zip\")\n",
    "with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "    for p in OUT_DIR.glob(\"*\"):\n",
    "        zf.write(p, arcname=p.name)\n",
    "\n",
    "print(\"Generación completa. Archivos en:\", OUT_DIR)\n",
    "print(\"ZIP:\", zip_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9e1a9",
   "metadata": {},
   "source": [
    "Este es un **script en Python** que puedes correr directamente en un cuaderno Jupyter abierto con **VS Code**. El script:\n",
    "\n",
    "1. Carga los archivos sintéticos (los `.csv` generados).\n",
    "2. Muestra un vistazo rápido a los datos.\n",
    "3. Realiza visualizaciones básicas con **matplotlib** y **seaborn**.\n",
    "4. Usa **pandas** para resumir semanal/mensual.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "264007c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================\n",
    "# Visualización de datos sintéticos: Caucasia\n",
    "# ============================================\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuración de estilo\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "sns.set_palette(\"viridis\")\n",
    "\n",
    "# -------------------------------\n",
    "# 1. Cargar datos meteorológicos\n",
    "# -------------------------------\n",
    "CARPETA =   r\"C:\\Users\\marco\\Documentos\\investigacion\\arima\\1_programa_trabajo\\2_ejecucion_plan_trabajo\\1_dataset_artificial_para_aprendizaje\\raw\"  \n",
    "ideam = pd.read_csv(CARPETA+\"\\meteorology_IDEAM_Caucasia_001.csv\", \\\n",
    "    parse_dates=[\"date\"])\n",
    "amsc = pd.read_csv(CARPETA+\"\\meteorology_MiraCielo_002.csv\", \\\n",
    "    parse_dates=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5db9214e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC-05:00]",
         "type": "unknown"
        },
        {
         "name": "tmin_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmean_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmax_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rh_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_m_s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0f74ecea-9a20-4418-8e9b-fba73e0a589f",
       "rows": [
        [
         "0",
         "2020-01-01 00:00:00-05:00",
         "16.98",
         "22.6",
         "26.05",
         "71.3",
         "5.0",
         "2.19",
         "IDEAM_Caucasia_001"
        ],
        [
         "1",
         "2020-01-02 00:00:00-05:00",
         "18.93",
         "21.84",
         "25.47",
         "68.8",
         "2.0",
         "4.53",
         "IDEAM_Caucasia_001"
        ],
        [
         "2",
         "2020-01-03 00:00:00-05:00",
         "18.63",
         "22.78",
         "26.17",
         "76.3",
         "6.0",
         "3.33",
         "IDEAM_Caucasia_001"
        ],
        [
         "3",
         "2020-01-04 00:00:00-05:00",
         "20.37",
         "23.84",
         "26.55",
         "63.9",
         "3.0",
         "1.34",
         "IDEAM_Caucasia_001"
        ],
        [
         "4",
         "2020-01-05 00:00:00-05:00",
         "17.39",
         "21.74",
         "25.87",
         "72.8",
         "6.0",
         "3.15",
         "IDEAM_Caucasia_001"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tmin_C</th>\n",
       "      <th>tmean_C</th>\n",
       "      <th>tmax_C</th>\n",
       "      <th>rh_pct</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>wind_m_s</th>\n",
       "      <th>station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00-05:00</td>\n",
       "      <td>16.98</td>\n",
       "      <td>22.60</td>\n",
       "      <td>26.05</td>\n",
       "      <td>71.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.19</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00-05:00</td>\n",
       "      <td>18.93</td>\n",
       "      <td>21.84</td>\n",
       "      <td>25.47</td>\n",
       "      <td>68.8</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.53</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00-05:00</td>\n",
       "      <td>18.63</td>\n",
       "      <td>22.78</td>\n",
       "      <td>26.17</td>\n",
       "      <td>76.3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.33</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00-05:00</td>\n",
       "      <td>20.37</td>\n",
       "      <td>23.84</td>\n",
       "      <td>26.55</td>\n",
       "      <td>63.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.34</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00-05:00</td>\n",
       "      <td>17.39</td>\n",
       "      <td>21.74</td>\n",
       "      <td>25.87</td>\n",
       "      <td>72.8</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.15</td>\n",
       "      <td>IDEAM_Caucasia_001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  tmin_C  tmean_C  tmax_C  rh_pct  precip_mm  \\\n",
       "0 2020-01-01 00:00:00-05:00   16.98    22.60   26.05    71.3        5.0   \n",
       "1 2020-01-02 00:00:00-05:00   18.93    21.84   25.47    68.8        2.0   \n",
       "2 2020-01-03 00:00:00-05:00   18.63    22.78   26.17    76.3        6.0   \n",
       "3 2020-01-04 00:00:00-05:00   20.37    23.84   26.55    63.9        3.0   \n",
       "4 2020-01-05 00:00:00-05:00   17.39    21.74   25.87    72.8        6.0   \n",
       "\n",
       "   wind_m_s          station_id  \n",
       "0      2.19  IDEAM_Caucasia_001  \n",
       "1      4.53  IDEAM_Caucasia_001  \n",
       "2      3.33  IDEAM_Caucasia_001  \n",
       "3      1.34  IDEAM_Caucasia_001  \n",
       "4      3.15  IDEAM_Caucasia_001  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ideam.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb4a5f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC-05:00]",
         "type": "unknown"
        },
        {
         "name": "tmin_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmean_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "tmax_C",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "rh_pct",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "precip_mm",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "wind_m_s",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "station_id",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "eb6b53b9-263a-4404-8e92-86e04963362f",
       "rows": [
        [
         "0",
         "2020-01-01 00:00:00-05:00",
         "16.72",
         "20.84",
         "23.12",
         "83.1",
         "4.0",
         "2.01",
         "MiraCielo_002"
        ],
        [
         "1",
         "2020-01-02 00:00:00-05:00",
         "16.52",
         "21.19",
         "24.51",
         "75.9",
         "5.0",
         "5.43",
         "MiraCielo_002"
        ],
        [
         "2",
         "2020-01-03 00:00:00-05:00",
         "20.71",
         "23.86",
         "28.03",
         "74.8",
         "4.0",
         "1.58",
         "MiraCielo_002"
        ],
        [
         "3",
         "2020-01-04 00:00:00-05:00",
         "18.06",
         "22.55",
         "26.46",
         "78.7",
         "3.0",
         "2.41",
         "MiraCielo_002"
        ],
        [
         "4",
         "2020-01-05 00:00:00-05:00",
         "17.24",
         "21.98",
         "26.65",
         "83.1",
         "2.0",
         "4.83",
         "MiraCielo_002"
        ]
       ],
       "shape": {
        "columns": 8,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>tmin_C</th>\n",
       "      <th>tmean_C</th>\n",
       "      <th>tmax_C</th>\n",
       "      <th>rh_pct</th>\n",
       "      <th>precip_mm</th>\n",
       "      <th>wind_m_s</th>\n",
       "      <th>station_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00-05:00</td>\n",
       "      <td>16.72</td>\n",
       "      <td>20.84</td>\n",
       "      <td>23.12</td>\n",
       "      <td>83.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.01</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00-05:00</td>\n",
       "      <td>16.52</td>\n",
       "      <td>21.19</td>\n",
       "      <td>24.51</td>\n",
       "      <td>75.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.43</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00-05:00</td>\n",
       "      <td>20.71</td>\n",
       "      <td>23.86</td>\n",
       "      <td>28.03</td>\n",
       "      <td>74.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.58</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00-05:00</td>\n",
       "      <td>18.06</td>\n",
       "      <td>22.55</td>\n",
       "      <td>26.46</td>\n",
       "      <td>78.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.41</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00-05:00</td>\n",
       "      <td>17.24</td>\n",
       "      <td>21.98</td>\n",
       "      <td>26.65</td>\n",
       "      <td>83.1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.83</td>\n",
       "      <td>MiraCielo_002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  tmin_C  tmean_C  tmax_C  rh_pct  precip_mm  \\\n",
       "0 2020-01-01 00:00:00-05:00   16.72    20.84   23.12    83.1        4.0   \n",
       "1 2020-01-02 00:00:00-05:00   16.52    21.19   24.51    75.9        5.0   \n",
       "2 2020-01-03 00:00:00-05:00   20.71    23.86   28.03    74.8        4.0   \n",
       "3 2020-01-04 00:00:00-05:00   18.06    22.55   26.46    78.7        3.0   \n",
       "4 2020-01-05 00:00:00-05:00   17.24    21.98   26.65    83.1        2.0   \n",
       "\n",
       "   wind_m_s     station_id  \n",
       "0      2.01  MiraCielo_002  \n",
       "1      5.43  MiraCielo_002  \n",
       "2      1.58  MiraCielo_002  \n",
       "3      2.41  MiraCielo_002  \n",
       "4      4.83  MiraCielo_002  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amsc.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a355a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 2. Cargar datos epidemiológicos\n",
    "# -------------------------------\n",
    "epi = pd.read_csv(CARPETA+\"\\epidemiology_caucasia_municipal.csv\", parse_dates=[\"date\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec018c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "date",
         "rawType": "datetime64[ns, UTC-05:00]",
         "type": "unknown"
        },
        {
         "name": "cases",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "hospitalizations",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "population",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "51472304-700e-4784-9c18-82eba87ecb91",
       "rows": [
        [
         "0",
         "2020-01-01 00:00:00-05:00",
         "6.0",
         "0",
         "140000"
        ],
        [
         "1",
         "2020-01-02 00:00:00-05:00",
         "4.0",
         "0",
         "140000"
        ],
        [
         "2",
         "2020-01-03 00:00:00-05:00",
         "5.0",
         "0",
         "140000"
        ],
        [
         "3",
         "2020-01-04 00:00:00-05:00",
         "6.0",
         "1",
         "140000"
        ],
        [
         "4",
         "2020-01-05 00:00:00-05:00",
         "9.0",
         "0",
         "140000"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>cases</th>\n",
       "      <th>hospitalizations</th>\n",
       "      <th>population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-01-01 00:00:00-05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-01-02 00:00:00-05:00</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2020-01-03 00:00:00-05:00</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2020-01-04 00:00:00-05:00</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020-01-05 00:00:00-05:00</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       date  cases  hospitalizations  population\n",
       "0 2020-01-01 00:00:00-05:00    6.0                 0      140000\n",
       "1 2020-01-02 00:00:00-05:00    4.0                 0      140000\n",
       "2 2020-01-03 00:00:00-05:00    5.0                 0      140000\n",
       "3 2020-01-04 00:00:00-05:00    6.0                 1      140000\n",
       "4 2020-01-05 00:00:00-05:00    9.0                 0      140000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epi.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2672e70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -------------------------------\n",
    "# 3. Resumen de los datasets\n",
    "# -------------------------------\n",
    "print(\"IDEAM head:\")\n",
    "display(ideam.head())\n",
    "\n",
    "print(\"Mira Cielo head:\")\n",
    "display(mira_cielo.head())\n",
    "\n",
    "print(\"Epidemiología head:\")\n",
    "display(epi.head())\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Gráficos meteorológicos\n",
    "# -------------------------------\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 10), sharex=True)\n",
    "\n",
    "sns.lineplot(data=ideam, x=\"date\", y=\"temperature\", ax=axes[0], label=\"Temperatura IDEAM\")\n",
    "axes[0].set_ylabel(\"°C\")\n",
    "\n",
    "sns.lineplot(data=ideam, x=\"date\", y=\"humidity\", ax=axes[1], label=\"Humedad IDEAM\")\n",
    "axes[1].set_ylabel(\"%\")\n",
    "\n",
    "sns.lineplot(data=ideam, x=\"date\", y=\"precipitation\", ax=axes[2], label=\"Precipitación IDEAM\")\n",
    "axes[2].set_ylabel(\"mm\")\n",
    "\n",
    "fig.suptitle(\"Datos Meteorológicos IDEAM - Caucasia (2020–2025)\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 5. Gráficos epidemiológicos\n",
    "# -------------------------------\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=epi, x=\"date\", y=\"cases\", ax=ax, label=\"Casos diarios\")\n",
    "sns.lineplot(data=epi, x=\"date\", y=\"hospitalizations\", ax=ax, label=\"Hospitalizaciones\")\n",
    "ax.set_ylabel(\"Número de casos\")\n",
    "ax.set_title(\"Epidemiología - Caucasia (2020–2025)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# -------------------------------\n",
    "# 6. Resúmenes agregados\n",
    "# -------------------------------\n",
    "# Resumen semanal\n",
    "epi_weekly = epi.resample(\"W\", on=\"date\").sum(numeric_only=True)\n",
    "print(\"Resumen epidemiológico semanal:\")\n",
    "display(epi_weekly.head())\n",
    "\n",
    "# Resumen mensual de IDEAM\n",
    "ideam_monthly = ideam.resample(\"M\", on=\"date\").mean(numeric_only=True)\n",
    "print(\"Resumen meteorológico mensual (IDEAM):\")\n",
    "display(ideam_monthly.head())\n",
    "\n",
    "# Visualización resumen mensual\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "sns.lineplot(data=ideam_monthly, x=ideam_monthly.index, y=\"temperature\", ax=ax, label=\"Temperatura media\")\n",
    "sns.lineplot(data=ideam_monthly, x=ideam_monthly.index, y=\"humidity\", ax=ax, label=\"Humedad media\")\n",
    "ax.set_ylabel(\"Valores promedio\")\n",
    "ax.set_title(\"Resumen mensual Meteorológico IDEAM\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f814d",
   "metadata": {},
   "source": [
    "```\n",
    "\n",
    "---\n",
    "\n",
    "📌 **Cómo usarlo en VS Code (Jupyter):**\n",
    "\n",
    "1. Coloca el script en una celda de tu cuaderno `.ipynb`.\n",
    "2. Asegúrate de que los `.csv` estén en el mismo directorio del cuaderno (o cambia las rutas en `pd.read_csv`).\n",
    "3. Ejecuta celda por celda para ver las tablas y gráficos.\n",
    "\n",
    "---\n",
    "\n",
    "¿Quieres que te adapte este mismo script para que genere un **dashboard interactivo** con `ipywidgets` (sliders para elegir rango de fechas, variables, etc.)?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae770ed5",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136505ef",
   "metadata": {},
   "source": [
    "\n",
    "# 2) Qué se genera (archivos)\n",
    "\n",
    "Al ejecutar el script que yo corrí aquí, el paquete generado contiene:\n",
    "\n",
    "* `meteorology_IDEAM_Caucasia_001.csv` — serie diaria 2020-01-01 → 2025-12-31 con tmin, tmean, tmax, rh\\_pct, precip\\_mm, wind\\_m\\_s y `station_id`.\n",
    "* `meteorology_MiraCielo_002.csv` — segunda estación con un hueco declarado (sensor change) en 2021-11-02 → 2021-11-12 (NaNs introducidos intencionalmente).\n",
    "* `epidemiology_caucasia_municipal.csv` — casos diarios y hospitalizaciones; incluye hueco en 2022-08-10 → 2022-08-17 para emular fallos en reporte.\n",
    "* `stations_metadata.csv` — metadatos de estaciones (lat/lon/elev/source/notes).\n",
    "* `inventory.json` — inventario con SHA256 para cada CSV, periodo, frecuencia y metadatos de procedencia.\n",
    "* `caucasia_synthetic_raw_2020_2025.zip` — ZIP con todos los archivos anteriores.\n",
    "\n",
    "En el entorno donde ejecuté el script (notebook), el ZIP quedó en:\n",
    "`/mnt/data/caucasia_synthetic_raw_2020_2025.zip`\n",
    "\n",
    "Y los archivos CSV en:\n",
    "`/mnt/data/raw/`\n",
    "\n",
    "Hashes mostrados (ejemplo generado):\n",
    "\n",
    "* meteorology\\_IDEAM\\_Caucasia\\_001.csv: `c49261e9...`\n",
    "* meteorology\\_MiraCielo\\_002.csv: `316d33b5...`\n",
    "* epidemiology\\_caucasia\\_municipal.csv: `97db3db1...`\n",
    "* stations\\_metadata.csv: `d16ce287...`\n",
    "\n",
    "(En ejecución local tus hashes cambiarán si replicas el proceso con distinta semilla.)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8aa88",
   "metadata": {},
   "source": [
    "\n",
    "# 3) Cómo usarlo en tu pipeline ETL\n",
    "\n",
    "Recomendaciones rápidas:\n",
    "\n",
    "* Integra este script como una etapa `01_generate_synthetic.py` dentro de `/data/raw/` para pruebas del pipeline.\n",
    "* Para pasar a la fase de limpieza (`02_clean.py`) apunta a `raw/*.csv` y usa `inventory.json` para validar integridad (hashes y periodos).\n",
    "* Mantén la semilla (`np.random.seed(...)`) si quieres reproducibilidad exacta entre ejecuciones.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5aac22e",
   "metadata": {},
   "source": [
    "\n",
    "# 4) ¿Quieres que haga algo más ahora?\n",
    "\n",
    "Puedo, según prefieras:\n",
    "\n",
    "* Ajustar la generación (más estaciones, variabilidad espacial, diferentes poblaciones municipales).\n",
    "* Incluir variables adicionales (radiación solar, velocidad del viento horaria, índice de vegetación).\n",
    "* Entregar el notebook ETL completo (descarga → limpieza → transformaciones → integración) listo para correr con estos archivos sintéticos.\n",
    "* Generar un conjunto de pruebas de calidad automáticas (checks) que validen rangos, continuidad, y alertas en `inventory.json`.\n",
    "\n",
    "Dime cuál de estas opciones quieres y lo hago ahora mismo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccac8ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a3f5b2a",
   "metadata": {},
   "source": [
    "\n",
    "## 2) Limpieza de Datos\n",
    "\n",
    "**Objetivo**: detectar y corregir faltantes, errores de medición y valores atípicos sin sesgar la señal temporal.\n",
    "\n",
    "**Pasos y técnicas**\n",
    "\n",
    "1. **Verificación inicial**\n",
    "\n",
    "   * Comprobar integridad de fechas (continuidad diaria), duplicados, y formatos.\n",
    "   * Identificar cambios en la estación (cambios de sensor/ubicación) usando metadatos.\n",
    "\n",
    "2. **Identificación de faltantes y anomalías**\n",
    "\n",
    "   * Estadísticas por variable: % de datos faltantes por estación/periodo.\n",
    "   * Visual: heatmap de presencia/ausencia por fecha.\n",
    "   * Identificar outliers con métodos combinados: IQR por ventana estacional, Z-score por temporada, y técnicas robustas como **Hampel filter** y **Isolation Forest** para outliers multivariados (meteorología + epidemiología).\n",
    "\n",
    "3. **Manejo de datos faltantes**\n",
    "\n",
    "   * **Pequeños huecos (1–3 días)**: interpolación temporal (lineal o spline) o **interpolación estacional** (por día del año promedio).\n",
    "   * **Huecos medianos (4–30 días)**: imputación por **modelo de series temporales** (ej. ETS o Kalman smoothing / state-space), o por regresión con estaciones cercanas (spatial kriging o regresión múltiple).\n",
    "   * **Huecos largos (>30 días)**: marcar como no imputados si la imputación es arriesgada; usar variables agregadas (semana/mes) donde sea viable.\n",
    "   * **Epidemiología – casos**: evitar imputaciones automáticas agresivas. Cuando falten reportes, documentar cambios de sistema de reporte; considerar imputación por distribución de Poisson/Gamma condicionada a tendencias de semanas previas y covariables meteorológicas sólo si está justificado.\n",
    "\n",
    "4. **Corrección de errores de medición**\n",
    "\n",
    "   * Validar rangos físicos (p. ej. temperatura plausible, humedad 0–100%). Valores fuera de rango → marcar como inválidos.\n",
    "   * Si hay saltos bruscos coincidentes con cambios de sensor, ajustar series por comparaciones con estaciones vecinas usando regresión para calibrar.\n",
    "\n",
    "5. **Registro de decisiones**\n",
    "\n",
    "   * Crear un log de limpieza (tipo) por fecha y registro (qué se imputó, método, justificativo).\n",
    "\n",
    "---\n",
    "\n",
    "## 3) Transformación de Datos\n",
    "\n",
    "**Objetivo**: preparar variables adecuadas para análisis y para ser usadas como exógenas en SARIMAX.\n",
    "\n",
    "**Transformaciones recomendadas**\n",
    "\n",
    "1. **Fechas y calendario**\n",
    "\n",
    "   * Convertir a índice de tiempo (DatetimeIndex) en pandas; crear columnas: `date`, `year`, `month`, `day`, `dow` (día semana), `doy` (día del año), `epiweek` (semana epidemiológica ISO) si usarás análisis semanal.\n",
    "\n",
    "2. **Variables derivadas meteorológicas**\n",
    "\n",
    "   * **Medias móviles**: 7, 14, 30 días (ej.: `temp_7d_mean`, `precip_14d_sum`).\n",
    "   * **Acumulados**: precipitación acumulada 7/14/30 días.\n",
    "   * **Índices**: *heat index* (temperatura + humedad), *evapotranspiración aproximada* (si tienes radiación o viento), `Tmin`, `Tmax` diarias.\n",
    "   * **Anomalías estacionales**: valor − media diurna histórica (por día del año), para resaltar episodios anómalos.\n",
    "   * **Indicadores discretos**: `had_precip` (booleano si lluvia > 0.1 mm).\n",
    "\n",
    "3. **Variables epidemiológicas**\n",
    "\n",
    "   * **Counts vs rates**: preferir tasas por 1000 habitantes cuando sea relevante; si solo tienes counts, incluir `population` para convertir.\n",
    "   * **Transformaciones**: `log(count + 1)` para estabilizar varianza; diferencia (diff) si modelas cambios.\n",
    "   * **Lags**: crear `cases_lag_1..lag_k` y lags meteorológicos (`precip_lag_0..lag_m`) hasta el máximo plausible (p. ej. 0–28 días) según biología del vector/enfermedad.\n",
    "   * **Promedios lagged**: media de temperatura lag 7–14 días (para efectos retrasados).\n",
    "\n",
    "4. **Escalado/normalización**\n",
    "\n",
    "   * Para selección de covariables y regularización: **standard scaler** (z-score) o **min-max**; mantener copias sin escalar para interpretabilidad de coeficientes.\n",
    "   * No escalar variables que vayas a usar en transformaciones dirigidas a SARIMAX si quieres interpretar magnitudes directamente; escalar cuando uses algoritmos de selección multivariada o regularizados.\n",
    "\n",
    "5. **Estacionariedad y transformaciones de Box–Cox**\n",
    "\n",
    "   * Probar **ADF** para stationarity; si no estacionaria, usar **diferenciación** (d, D) y/o Box–Cox (registrar lambda).\n",
    "\n",
    "---\n",
    "\n",
    "## 4) Integración de Datos\n",
    "\n",
    "**Objetivo**: producir un dataset final con índice temporal consistente y variables debidamente alineadas para SARIMAX (endógena + exógenas).\n",
    "\n",
    "**Pasos concretos**\n",
    "\n",
    "1. **Unificar frecuencia**\n",
    "\n",
    "   * Mantener **diaria**; si algunas series son horarias, agregarlas (media/suma) a diario. Si epidemiología solo está semanal, mantener diaria con NaN diarios y luego agregar (o expandir semanal a diario con la misma tasa — documentar). Para SARIMAX es preferible tener la misma frecuencia en endógena y exógenas.\n",
    "\n",
    "2. **Alineamiento temporal**\n",
    "\n",
    "   * **Join** por fecha (left-join usando la serie endógena como referencia).\n",
    "   * Para datos con frecuencia distinta: `resample('D').sum()` o `.mean()` según contexto. Para pasar de diaria a semanal usar `resample('W-MON')` o `W-SUN` según definición de semana epidemiológica.\n",
    "\n",
    "3. **Manejo de desfases biológicos**\n",
    "\n",
    "   * Construir una **matriz de diseño** de exógenas con múltiples lags y agregados (p. ej. `precip_lag_7`, `temp_14d_mean`) para explorar retardos en EDA y en selección de modelo.\n",
    "   * Procurar no incluir lags redundantes que generen multicolinealidad extrema; usar selección por VIF o PCA cuando sea necesario.\n",
    "\n",
    "4. **Coherencia y validación**\n",
    "\n",
    "   * Verificar que después de la integración no hayan saltos no explicados (p. ej. cuando faltó toda una semana en meteorología).\n",
    "   * Comprobar correlaciones temporales y que las fechas de eventos (picos epidémicos) coincidan con la información de notificación.\n",
    "\n",
    "5. **Formato de salida**\n",
    "\n",
    "   * Guardar dataset consolidado en **Parquet** (compacto, mantiene tipos) y en **CSV** para interoperabilidad.\n",
    "   * Acompañar con `metadata.yml` que describa variables, unidades, método de imputación y versión.\n",
    "\n",
    "---\n",
    "\n",
    "## 5) Análisis Exploratorio de Datos (EDA)\n",
    "\n",
    "**Objetivo**: entender patrones estacionales, tendencias, relaciones y retardos entre meteorología y epidemiología antes de modelar.\n",
    "\n",
    "**Técnicas y visualizaciones recomendadas**\n",
    "\n",
    "1. **Series temporales**\n",
    "\n",
    "   * Gráficas de la serie endógena (casos, hospitalizaciones) superpuestas con principales exógenas (temp, precip) — plot diario y smoothing (LOESS).\n",
    "   * Gráficas desagregadas por año para visualizar estacionalidad.\n",
    "\n",
    "2. **Descomposición**\n",
    "\n",
    "   * `seasonal_decompose` (additive/multiplicative) o STL para separar tendencia, estacionalidad y residuo.\n",
    "\n",
    "3. **Autocorrelaciones**\n",
    "\n",
    "   * ACF/PACF de la serie endógena y de los residuos iniciales (para sugerir órdenes ARIMA p,q y diferencias).\n",
    "   * CCF (cross-correlation function) entre casos y cada meteorológica para detectar retardos con máxima correlación (ej.: precip lag 10).\n",
    "\n",
    "4. **Correlaciones y lag plots**\n",
    "\n",
    "   * Heatmap de correlación entre variables y entre lags (matriz corr de `cases` vs `temp_lag_0..28`, etc.).\n",
    "   * Scatter plots y *lag scatter plots* para patrones no lineales.\n",
    "\n",
    "5. **Estadísticas descriptivas**\n",
    "\n",
    "   * Media, mediana, desviación, percentiles, % de ceros (precip), tasas por 1000 hab.\n",
    "   * Distribuciones (histogramas) y QQ-plots para verificar normalidad (útil para decidir transformaciones).\n",
    "\n",
    "6. **Identificación de rupturas estructurales**\n",
    "\n",
    "   * Pruebas de cambio de régimen (CUSUM, Chow test) si fue implementado un cambio en reportes o intervención sanitaria.\n",
    "\n",
    "7. **Análisis de colinealidad**\n",
    "\n",
    "   * VIF para exógenas; si VIF alto (>10) considerar combinar variables (PCA) o eliminar.\n",
    "\n",
    "8. **Pruebas causales exploratorias**\n",
    "\n",
    "   * **Granger causality tests** para ver si meteorología “granger-predice” casos (no es prueba causal definitiva, pero orienta lags).\n",
    "\n",
    "9. **Visuales recomendados** (resumen)\n",
    "\n",
    "   * Time series plots (endógena + exógenas con ejes doble).\n",
    "   * STL decomposition.\n",
    "   * ACF/PACF y CCF.\n",
    "   * Heatmap de correlación lags.\n",
    "   * Boxplots por mes/estación (para ver estacionalidad).\n",
    "   * Mapas (si hay variación espacial entre estaciones).\n",
    "\n",
    "10. **Selección preliminar de covariables**\n",
    "\n",
    "* Basada en: fuerza de correlación en los lags biológicamente plausibles, significado epidemiológico, VIF y parsimonia.\n",
    "\n",
    "---\n",
    "\n",
    "## 6) Documentación y Reproducibilidad\n",
    "\n",
    "**Objetivo**: asegurar que cualquier investigador pueda reproducir la extracción, limpieza y consolidación, y que el pipeline sea auditable y versionable.\n",
    "\n",
    "**Prácticas obligatorias**\n",
    "\n",
    "1. **Control de versiones**\n",
    "\n",
    "   * Repositorio Git (GitHub/GitLab) con ramas `main`, `dev`.\n",
    "   * Commits claros; usar *issues* y *pull requests* para cambios mayores.\n",
    "\n",
    "2. **Pipeline reproducible**\n",
    "\n",
    "   * Organizar en carpetas: `/data/raw`, `/data/processed`, `/notebooks`, `/src`, `/docs`.\n",
    "   * Scripts ETL en `/src` (por ejemplo `01_fetch.py`, `02_clean.py`, `03_transform.py`, `04_integrate.py`) ejecutables en orden.\n",
    "   * Uso de **Makefile** o **Snakemake** para orquestar pasos y dependencias.\n",
    "\n",
    "3. **Entorno**\n",
    "\n",
    "   * `environment.yml` (conda) o `requirements.txt` (pip) y `Dockerfile` para reproducibilidad de entorno.\n",
    "   * Versionar librerías críticas (pandas, statsmodels, scikit-learn).\n",
    "\n",
    "4. **Notebooks y pruebas**\n",
    "\n",
    "   * Notebooks Jupyter para EDA y reportes; scripts para ETL.\n",
    "   * Tests unitarios simples para funciones críticas (por ejemplo: test de integridad de fechas, test de no duplicados), en `tests/`.\n",
    "\n",
    "5. **Metadatos y registros**\n",
    "\n",
    "   * `metadata.yml` explicando variables, unidades, origen, método de imputation y cambios de versión de datos.\n",
    "   * `provenance.log` o `data_catalog.csv` con hashes de archivos originales, fecha extracción, y persona responsable.\n",
    "\n",
    "6. **Documentos y reportes**\n",
    "\n",
    "   * Generar informes reproducibles (nbconvert, papermill, or GitHub Actions) que creen reportes PDF/HTML con EDA y resumen de calidad.\n",
    "   * Mantener un changelog (CHANGELOG.md) para la base de datos y los modelos.\n",
    "\n",
    "7. **Políticas y ética**\n",
    "\n",
    "   * Si datos epidemiológicos son sensibles: anonimizar, agregar por fecha/área mínima, revisar permisos y políticas de protección de datos (Ley de Protección de Datos en Colombia).\n",
    "   * Guardar y documentar consentimiento/uso cuando aplique.\n",
    "\n",
    "8. **Entrega y preservación**\n",
    "\n",
    "   * Versión final del dataset (por ejemplo: `caucasia_timeseries_v1.0.parquet`) con DOI si se publica (Zenodo / institutional repo).\n",
    "   * Backup regular y control de acceso (roles).\n",
    "\n",
    "---\n",
    "\n",
    "## Apéndice práctico (resumen técnico y recomendaciones concretas)\n",
    "\n",
    "* **Frecuencia**: conservar diaria, agregar semanal/mes según necesidad.\n",
    "* **Periodo de análisis**: objetivo ≥ 10–16 años (2007–2024 recomendado).\n",
    "* **Imputación preferida**: pequeña—interpolación; mediana—Kalman smoothing o model-based; grande—usar datos de estaciones vecinas o marcar como no confiable.\n",
    "* **Outliers**: combinar filtros estadísticos (IQR/Hampel) y técnicas de ML (Isolation Forest) para detección; manual review cuando afecte picos epidémicos.\n",
    "* **Formato de almacenamiento**: Parquet + metadata YAML; raw en CSV/zip.\n",
    "* **Pipeline**: scripts versionados + Docker + pruebas automatizadas.\n",
    "* **Validación de modelos**: usar validación temporal (rolling-origin / expanding window) y métricas RMSE/MAE/MAPE; revisar residuos (no autocorrelación, homocedasticidad).\n",
    "* **Selección exógenas para SARIMAX**: basarse en EDA (CCF + plausibilidad biológica); controlar multicolinealidad; probar modelos con distintos sets de lags y seleccionar por AIC/BIC y performance out-of-sample.\n",
    "\n",
    "---\n",
    "\n",
    "## Consejos operativos finales y decisiones prácticas\n",
    "\n",
    "1. **Empieza por un “proyecto piloto”** con 3–5 años de datos completos para validar pipeline y decisiones de imputación antes de aplicar al archivo histórico completo.\n",
    "2. **Documenta cada decisión** en el `provenance.log` (p. ej. “imputé 2012-03-05..2012-03-08 precip con spline debido falla de sensor; fuente estación X”).\n",
    "3. **Mantén comunicación activa con el personal local** (Secretaría de Salud y técnicos IDEAM) para conocer cambios de protocolo y mantenimiento de estaciones.\n",
    "4. **Automatiza** extracciones periódicas y notificaciones de calidad (p. ej. alerta si % faltantes diario > 5%).\n",
    "\n",
    "---\n",
    "\n",
    "Si quieres, te puedo entregar lo siguiente **listo para ejecutar** (elige uno o varios):\n",
    "\n",
    "* Un **notebook Jupyter** (Python/pandas/statsmodels) con un pipeline ETL mínimo (descarga-sintética → limpieza → transformaciones → dataset final) y ejemplos de EDA y tests ADF/ACF para SARIMAX.\n",
    "* Plantillas `metadata.yml`, `provenance.log` y `Makefile`/`Dockerfile` para producción.\n",
    "* Un esquema de **matriz de lags** sugerida (precip/temp/humidity con lags 0–28 y medias móviles 7/14/30) listo para usar.\n",
    "\n",
    "¿Quieres que te genere el notebook ETL ejemplo y las plantillas comentadas para tu proyecto en Caucasia (lo creo en formato Jupyter listo para ejecutar)? Puedo prepararlo inmediatamente.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651548f",
   "metadata": {},
   "source": [
    "# Metodología para tratamiento de los datos en esta investigación \n",
    "\n",
    "1. Generacion los datos meteorológicos y epidemiológicos que emulen los de [AMSC](https://www.antioquiamirasucielo.org/): \n",
    "2. Generacion los datos meteorológicos y epidemiológicos que emulen los de NASA POWER:   \n",
    "3. determinar los rezagos de cada variable. Es decir, los $k_{i}$ de cada variable $X_{i}$.\n",
    "4. Transformar los datos en datos que tienen en cuenta tales rezagos. Es decir, que las variables ya queden de la forma $X_{i, t-k_{i}}$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2a46cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
